<TeXmacs|2.1.2>

<style|<tuple|beamer|chinese|rough-paper>>

<\body>
  <screens|<\hidden>
    \;

    \;

    <\with|par-mode|center>
      Summary of three papers regarding low-level vision

      \;
    </with>
  </hidden>|<\shown>
    <unroll|<\shown>
      <em|Learning a Deep Convolutional Network for Image Super-Resolution>

      \;
    </shown>|<\hidden*>
      deep CNN in 2014
    </hidden*>|<\hidden*>
      good quality and good efficiency
    </hidden*>|<\hidden*>
      subsumes sparse-coding-based methods
    </hidden*>>
  </shown>|<\hidden>
    <unroll|<\shown>
      <em|Networks are Slacking Off: Understanding Generalization Problem in
      Image Deraining>

      \;
    </shown>|<\hidden*>
      explain why sometimes more training data leads to worse performance
    </hidden*>|<\hidden*>
      data scale and data complexity
    </hidden*>|<\hidden*>
      learn reconstruction vs. learn degradation
    </hidden*>|<\hidden*>
      balance
    </hidden*>|<\hidden*>
      shortcut = local minimum?
    </hidden*>>
  </hidden>|<\hidden>
    <unroll|<\shown>
      <em|DegAE: A New Pretraining Paradigm for Low-level Vision>

      \;
    </shown>|<\hidden*>
      separate low-cost tasks and high-cost tasks in low-level vision
    </hidden*>|<\hidden*>
      explan why pretraing+fine-tuning is not suited for low-cost tasks
    </hidden*>|<\hidden*>
      a new approach to make sure the autoencoder learns the semantic
    </hidden*>>
  </hidden>|<\hidden>
    \;
  </hidden>|<\hidden>
    \;
  </hidden>|<\hidden>
    \;
  </hidden>>
</body>

<\initial>
  <\collection>
    <associate|page-medium|beamer>
  </collection>
</initial>